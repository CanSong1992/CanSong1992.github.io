---
layout: post
title: 理解LSTM网络
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

原文：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
## 循环神经网络
&emsp;&emsp;人类每一次思考不都是从头开始的。当你读这篇文章的时候，你读的每一个词都是基于以前每一个词的理解。所有的记忆都不会丢掉从头开始思考，思维都保存下来了。

&emsp;&emsp;传统的神经网络不能做到这些，他有一个主要的缺陷。例如，想象一下，你想要把一场电影中每一个点正在发生的事搞清楚。传统的神经网络很难从电影先前的事件从推断出后来的事情。

&emsp;&emsp;循环神经网络解决了这个问题。这种神经网络带有环，能够保存信息。

<div align = "center" style="margin-left:10%;margin-right:10%;padding:20px 30% 20px 30%;background-color:#666;"><!--上 右 下 左-->
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png" />
</div>
<p align = "center">循环神经网络带环</p>


&emsp;&emsp;在上图中，神经网络中的方框\\(A\\)输入\\(x_t\\),输出\\(h_t\\)。网络中的环总是让信息从某一时刻流向下一时刻。

&emsp;&emsp;这些环似乎成了循环神经网络的一个迷。然而，如果你多思考一下，它们并不比传统的神经网络复杂很多。一个循环神经网络可以看成是多个相同网络的复制，每一个节点把信息传给下一个。思考一下我们把环展开的时候发生了什么：

<div style="margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png"/>
</div>
<p align="center">展开的神经网络</p>

&emsp;&emsp;这个链状的结构揭示了循环神经网络与序列和列表密切相关的本质。它们是处理数据的神经网络的自然结构。

&emsp;&emsp;这无疑是有用的！在早些年，RNNs非常成功的应用到了这些方面：语音识别，语言建模，翻译，图像字幕……还有很多方面。我将就Andrej Karpathy优秀的博客[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)讨论他用RNNs实现的惊人成果。这真的很有意义。

&emsp;&emsp;这些成功的本事是“LSTMs”的使用，“LSTMs”一种特殊的循环神经网络，对于很多问题，它比标准的神经网络表现得更好。几乎所有基于循环神经网络的激动人心的成果都是用它实现的。这也是写这篇文章的原因。

## 长时依赖问题
&emsp;&emsp;RNNs吸引人的原因之一是它能把先前的信息连接到当前的任务，例如把先前的视频帧的信息用于当前帧的理解。如果RNNs能做这些，它将是非常有用的。但是它能吗？并不能。

&emsp;&emsp;有些时候，我们仅仅需要寻找最近的信息去完成当前任务。例如，构建一个用先前信息预测下一个单词的模型。如果我们想要预测句子“The clouds are in the __*sky*__”中最后的词。我们不需要任何前面的语境——显然接下来的词就是***sky***。在这中情况下，所需要的相关信息和它的位置之间的间隔是很小的，RNNs能学会使用先前的信息。

<div style="margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;"> 
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png"/>
</div>

&emsp;&emsp;但是有些场景也需要更多的上下文信息。思考预测文本“I grew up in French……I speek fluent *French*”中最后一个单词，最近的信息表明接下来的词可能是一种语言的名字，但是我们想要搞清楚是哪一种语言，我们需要很靠前的上文Franch。很可能相关信息和自身位置之间的间隔必须是很大的。
&emsp;&emsp;不幸的是，当这个间隔变得很大时，RNNs将很难学会联系相关信息。

<div style="margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;"> 
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png"/>
</div>

&emsp;&emsp;理论上，RNNs绝对能处理这种“长时间依赖”问题。人们能在这种形式中为解决这种无用的问题细心挑取参数。不幸的是，事实上RNNs似乎并不能学习到这些。[Hochreiter(1991)[German]](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)和[Bengio,et al.(1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)在深度上探索了这个问题。他们发现了一些很基本的困难之处。

&emsp;&emsp;幸运的是，LSTMs不存在这个问题。

## LSTM网络
&emsp;&emsp;长期记忆网络——通常称作“LSTM”——是一种特殊的RNN，它能学到长期的依赖关系。在下面的工作中<a href="#fn1" id="fnref1">\\(^{[1]}\\)</a>，[Hochreiter&Schmidhuber(1997)](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)引入并定义了LSTM，并受到了很多人的欢迎。它们在大量的问题上能很好的工作并且被广泛使用。

&emsp;&emsp;LSTMs的设计明确用于避免长期依赖问题。记住长期的信息几乎是它们默认的行为，学习起来毫不费劲！

&emsp;&emsp;所有的循环神经网络都是这种形式，链状的重复的神经网络模型。在标准的RNNs中，这种重复的模型是一个非常简单的结构，例如一个单独的tanh层。

<div style="margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png"/>
</div>
<p align="center">在循环神经网络中单层重复模块</p>

&emsp;&emsp;LSMTs也是这种结构的链状模型，但是重复的模块具有不同的结构。不同于单独的神经网络层，它们分为四部分，以一种特殊的方式相互影响。

<div style="margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png"/>
</div>
<p align="center">LSTM中包含四个相互作用层的重复模块</p>

&emsp;&emsp;不用害怕这种网络复杂的细节。我们将会一步一步的搞清楚LSTM模型。从现在开始，我们要适应我们将会使用的符号。
<div style="margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png"/>
</div>

&emsp;&emsp;在上面的图示中，每条线传递一个矢量，从一个节点的输出到另一个节点的输入。每个粉红色的点表示一个相应的操作，比如矢量加法，黄色的方框是用于学习的神经网络层。汇聚线表示连接，分叉线表示复制，并且复制的矢量将要流向不同的位置。

## LSTMs蕴含的核心思想
&emsp;&emsp;LSMTs的关键在于cell状态，在下图顶部通过的水平线。cell状态像一个传送带。沿着整个链都是笔直的，仅仅会受到一些小的直线的影响。它很容易让信息沿着它流动并且保持不变。
<div style="margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png"/>
</div>
&emsp;&emsp;LSMT根本不能给cell状态移除或添加信息，慢慢地通过门的结构来调节。

&emsp;&emsp;门是一种选择性的让信息通过的方式。它们由一个sigmoid神经网络层和一个对应的乘法操作组成。
<div style="margin-left:10%;margin-right:10%;padding:20px 30% 20px 30%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png"/>
</div>
&emsp;&emsp;sigmoid层输出的数据在0和1之间，描述的是每个元素允许通过的量。0表示全都不允许通过，1表示允许全部通过。

&emsp;&emsp;一个LSTM有三个这样的门，保存和控制cell状态。
## 一步一步理解LSTM
&emsp;&emsp;LSTM的第一步是决定来自于cell状态的什么信息被允许通过。这个决策是通过一个称作“忘记门层”的sigmoid层制定的。它输入*\\(h_{t-1}\\)*和*\\(x_t\\)*,并且为cell状态中的每一个值输出一个0到1之间的数字。1表示完全保持，0表示完全阻止。

&emsp;&emsp;让我们回到从基于先前的所有信息预测下一个词的语言模型例子上。cell状态可能包含当前话题的类别，因而对应的名词可能被使用。当我们开始一个新的话题，我们想要忘记原来话题的类型。
<div style="margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png"/>
</div>

&emsp;&emsp;接下来是决定什么信息将要被存在cell状态。这分为两部分，首先，一个成为“输入门层”的sigmoid层决定我们将要更新哪些值，其次，一个tanh层产生了一个新的可能被添加到cell状态的候选值，*\\(\widetilde{C_t}\\)*。接下来我们将结合这两者更新cell状态。

&emsp;&emsp;在我们的语言模型的例子中，我们想要添加新的主题来替代我们想要忘记的老的主题。

<div style="margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png"/>
</div>

&emsp;&emsp;现在该我们更新旧的cell状态*\\(C_{t-1}\\)*,生成新的cell状态*\\(C_t\\)*了。先前的步骤决定了怎么做，我们只需要做。

&emsp;&emsp;我们用*\\(f_t\\)*乘以旧的cell状态，忘记我们决定忘掉的先前的事，然后加上*\\(i_t\*\widetilde{C_t}\\)*。这是一个候选值，由我们想要更新每个一个cell状态值多少来决定。

&emsp;&emsp;在语言模型的例子中，这是我们丢掉老话题的信息，添加新信息类型的地方，正如我们前面决定的。

<div style="margin-left:10%;margin-right:10%;padding:20px 0% 20px 5%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png"/>
</div>

&emsp;&emsp;最后，我们需要决定我们将要输出什么。这个输出将依赖于cell状态，但是是过滤后的值。首先，我们生成一个sigmoid层来决定我们将要输出cell状态的哪一部分。然后我们让cell状态通过tanh层（将cell状态压缩到-1和1之间），并且乘以sigmoid门的输出，以此仅仅输出我们想要输出的部分。

&emsp;&emsp;对于语言模型的例子，当它看到一个主语，它可能想要与动词相关的信息，来推断下一步将要发生的事情。例如，它可能输出主语是是单数或复数，因而我们知道接下来动词应该是什么形式。

<div style="margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png"/>
</div>

## LSTM的变形
&emsp;&emsp;以上我们说的都是标准的LSTM模型。但是不是所有的LSTMs都是和上面的相同的。事实上，大多数与LSTMs相关的论文都是使用一个更简单的版本。难度是更小的，但是它们中 一些是值得提及的。
<div style="margin-left:10%;margin-right:10%;padding:20px 5% 20px 5%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png"/>
</div>

&emsp;&emsp;上面的图示为每一个门添加了窥视孔，但是很多论文只把窥视孔给其中一部分而另一部分不给。

&emsp;&emsp;另一种变体是使用对偶的忘记门和输入门。而不是分开决定忘记什么和添加什么新信息，这俩决策是一起制定的。我们仅仅忘记我们将要输入信息的地方。我们只在往我们忘记旧的东西的时候输入新的东西。

<div style="margin-left:10%;margin-right:10%;padding:20px 5% 20px 5%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png"/>
</div>

&emsp;&emsp;一个更大变化的LSTM变体是门控循环单元（GRU），由[Cho,et al.(2014)](http://arxiv.org/pdf/1406.1078v3.pdf)引进。他把忘记门和输入门融合成单独一个“更新门”。这也融合了cell状态和hidden状态，并且做了一些其他的改变。这个模型比标准LSTM模型更简单，并很快流行起来。

<div style="margin-left:10%;margin-right:10%;padding:20px 5% 20px 5%;background-color:#666;">
	<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png"/>
</div>


&emsp;&emsp;这仅仅是一些最引人注意的LSTM变体。还有很多其他的，比如[Yao,et al.(2015)](http://arxiv.org/pdf/1508.03790v2.pdf)的**深度门控RNNs（Depth Gated RNNs）**。还有一些用来处理长期依赖问题的不同的方法，比如[Koutnik, et al.](http://arxiv.org/pdf/1402.3511v1.pdf)的**Clockwork RNNs**。

&emsp;&emsp;这些变体的哪一种是最好的呢？有什么关键的区别吗？[Greff, et al.(2015)](http://arxiv.org/pdf/1503.04069.pdf)对一些流行的变体做了很好的比较，发现它们都是相同的。[Jozefowicz, et al.(2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)测试了数以万计的结构，发现其中的一些比LSTMs在某些任务上效果更好。

## 结论

&emsp;&emsp;先前我提到了一些用RNNs实现的显著的效果。本质上这些都能用LSTMs来实现。LSTMs确实对解决很多问题都很有效。

&emsp;&emsp;LSTMs的公式写出来看起来非常吓人，然而，在这篇文章中一步一步的分析它们就变得更容易理解了。

&emsp;&emsp;在RNNs的应用上，LSTMs迈出了很大一步。我们很自然的会想，还有更大的一步么？很多研究着门一个共同的想法是：“是的，还有更大的一步并且很吸引人！”这一步指的是让RNN从具有更多信息的集合中挑选信息。这可能需要选出图像的一部分输出对应的信息。事实上，[Xu, et al.(2015)](http://arxiv.org/pdf/1502.03044v2.pdf)恰恰在做这个——如果你想要开发显著性，这个可能会是一个很好的开端！使用显著性会产生一些非常激动人心的结果，这似乎正在向我们走来……

&emsp;&emsp;在RNN的研究中显著性不仅仅是激动人心，意义深远。例如，[Kalchbrenner, et al. (2015)](http://arxiv.org/pdf/1507.01526v1.pdf)的网状LSTMs似乎非常有前途。在生成模型中使用RNNs——例如[Gregor, et al. (2015)](http://arxiv.org/pdf/1502.04623.pdf), [Chung,, et al. (2015)](http://arxiv.org/pdf/1506.02216v3.pdf), [Bayer & Osendorfer (2015)](http://arxiv.org/pdf/1411.7610v3.pdf)——也非常有意思。对于循环神经网络来说，最近这些年是非常激动人心的，未来也充满希望。

## 致谢

&&emsp;&emsp;我很感谢很多人帮助我更好的理解了LSTMs，可视化的评论，对这篇博文提供了反馈。非常感谢google的同事非常友好的反馈，尤其是[Oriol Vinyals](http://research.google.com/pubs/OriolVinyals.html), [Greg Corrado](http://research.google.com/pubs/GregCorrado.html), [Jon Shlens](http://research.google.com/pubs/JonathonShlens.html), [Luke Vilnis](http://people.cs.umass.edu/~luke/)和[Ilya Sutskever](http://www.cs.toronto.edu/~ilya/)。同时也感谢其他花时间来帮助我的朋友和同事，包括[Dario Amodei](https://www.linkedin.com/pub/dario-amodei/4/493/393), [Jacob Steinhardt](http://cs.stanford.edu/~jsteinhardt/)。尤其是[Kyunghyun Cho](http://www.kyunghyuncho.me/)对我的图示给予了非常具有思考价值的回应。

&emsp;&emsp;最后，在两个研讨会上，我讨论了神经网络并练习去解释LSTMs。很感谢每个人耐心的参与并给了建议。

---

<li id="fn1"><p>[1].&emsp;除了原有作者，还有很多人对LSTM的改进做出了贡献。所列不全：Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo和 <a href="https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;hl=en">Alex Graves</a>.<a href="#fnref1">↩</a></p></li>


