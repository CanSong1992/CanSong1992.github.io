<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-04-11T09:24:16+08:00</updated><id>http://localhost:4000/</id><title type="html">Can Song</title><subtitle>Can' Home</subtitle><entry><title type="html">理解LSTM网络</title><link href="http://localhost:4000/Understanding-LSTM-Networks/" rel="alternate" type="text/html" title="理解LSTM网络" /><published>2017-04-10T00:00:00+08:00</published><updated>2017-04-10T00:00:00+08:00</updated><id>http://localhost:4000/Understanding-LSTM-Networks</id><content type="html" xml:base="http://localhost:4000/Understanding-LSTM-Networks/">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;原文：&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;循环神经网络&quot;&gt;循环神经网络&lt;/h2&gt;
&lt;p&gt;  人类每一次思考不都是从头开始的。当你读这篇文章的时候，你读的每一个词都是基于以前每一个词的理解。所有的记忆都不会丢掉从头开始思考，思维都保存下来了。&lt;/p&gt;

&lt;p&gt;  传统的神经网络不能做到这些，他有一个主要的缺陷。例如，想象一下，你想要把一场电影中每一个点正在发生的事搞清楚。传统的神经网络很难从电影先前的事件从推断出后来的事情。&lt;/p&gt;

&lt;p&gt;  循环神经网络解决了这个问题。这种神经网络带有环，能够保存信息。&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-left:10%;margin-right:10%;padding:20px 30% 20px 30%;background-color:#666;&quot;&gt;&lt;!--上 右 下 左--&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png&quot; /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;循环神经网络带环&lt;/p&gt;

&lt;p&gt;  在上图中，神经网络中的方框\(A\)输入\(x_t\),输出\(h_t\)。网络中的环总是让信息从某一时刻流向下一时刻。&lt;/p&gt;

&lt;p&gt;  这些环似乎成了循环神经网络的一个迷。然而，如果你多思考一下，它们并不比传统的神经网络复杂很多。一个循环神经网络可以看成是多个相同网络的复制，每一个节点把信息传给下一个。思考一下我们把环展开的时候发生了什么：&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png&quot; /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;展开的神经网络&lt;/p&gt;

&lt;p&gt;  这个链状的结构揭示了循环神经网络与序列和列表密切相关的本质。它们是处理数据的神经网络的自然结构。&lt;/p&gt;

&lt;p&gt;  这无疑是有用的！在早些年，RNNs非常成功的应用到了这些方面：语音识别，语言建模，翻译，图像字幕……还有很多方面。我将就Andrej Karpathy优秀的博客&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;讨论他用RNNs实现的惊人成果。这真的很有意义。&lt;/p&gt;

&lt;p&gt;  这些成功的本事是“LSTMs”的使用，“LSTMs”一种特殊的循环神经网络，对于很多问题，它比标准的神经网络表现得更好。几乎所有基于循环神经网络的激动人心的成果都是用它实现的。这也是写这篇文章的原因。&lt;/p&gt;

&lt;h2 id=&quot;长时依赖问题&quot;&gt;长时依赖问题&lt;/h2&gt;
&lt;p&gt;  RNNs吸引人的原因之一是它能把先前的信息连接到当前的任务，例如把先前的视频帧的信息用于当前帧的理解。如果RNNs能做这些，它将是非常有用的。但是它能吗？并不能。&lt;/p&gt;

&lt;p&gt;  有些时候，我们仅仅需要寻找最近的信息去完成当前任务。例如，构建一个用先前信息预测下一个单词的模型。如果我们想要预测句子“The clouds are in the &lt;strong&gt;&lt;em&gt;sky&lt;/em&gt;&lt;/strong&gt;”中最后的词。我们不需要任何前面的语境——显然接下来的词就是&lt;strong&gt;&lt;em&gt;sky&lt;/em&gt;&lt;/strong&gt;。在这中情况下，所需要的相关信息和它的位置之间的间隔是很小的，RNNs能学会使用先前的信息。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt; 
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  但是有些场景也需要更多的上下文信息。思考预测文本“I grew up in French……I speek fluent &lt;em&gt;French&lt;/em&gt;”中最后一个单词，最近的信息表明接下来的词可能是一种语言的名字，但是我们想要搞清楚是哪一种语言，我们需要很靠前的上文Franch。很可能相关信息和自身位置之间的间隔必须是很大的。
  不幸的是，当这个间隔变得很大时，RNNs将很难学会联系相关信息。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt; 
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  理论上，RNNs绝对能处理这种“长时间依赖”问题。人们能在这种形式中为解决这种无用的问题细心挑取参数。不幸的是，事实上RNNs似乎并不能学习到这些。&lt;a href=&quot;http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf&quot;&gt;Hochreiter(1991)[German]&lt;/a&gt;和&lt;a href=&quot;http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf&quot;&gt;Bengio,et al.(1994)&lt;/a&gt;在深度上探索了这个问题。他们发现了一些很基本的困难之处。&lt;/p&gt;

&lt;p&gt;  幸运的是，LSTMs不存在这个问题。&lt;/p&gt;

&lt;h2 id=&quot;lstm网络&quot;&gt;LSTM网络&lt;/h2&gt;
&lt;p&gt;  长期记忆网络——通常称作“LSTM”——是一种特殊的RNN，它能学到长期的依赖关系。在下面的工作中&lt;a href=&quot;#fn1&quot; id=&quot;fnref1&quot;&gt;\(^{[1]}\)&lt;/a&gt;，&lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&quot;&gt;Hochreiter&amp;amp;Schmidhuber(1997)&lt;/a&gt;引入并定义了LSTM，并受到了很多人的欢迎。它们在大量的问题上能很好的工作并且被广泛使用。&lt;/p&gt;

&lt;p&gt;  LSTMs的设计明确用于避免长期依赖问题。记住长期的信息几乎是它们默认的行为，学习起来毫不费劲！&lt;/p&gt;

&lt;p&gt;  所有的循环神经网络都是这种形式，链状的重复的神经网络模型。在标准的RNNs中，这种重复的模型是一个非常简单的结构，例如一个单独的tanh层。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png&quot; /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;在循环神经网络中单层重复模块&lt;/p&gt;

&lt;p&gt;  LSMTs也是这种结构的链状模型，但是重复的模块具有不同的结构。不同于单独的神经网络层，它们分为四部分，以一种特殊的方式相互影响。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png&quot; /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;LSTM中包含四个相互作用层的重复模块&lt;/p&gt;

&lt;p&gt;  不用害怕这种网络复杂的细节。我们将会一步一步的搞清楚LSTM模型。从现在开始，我们要适应我们将会使用的符号。&lt;/p&gt;
&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  在上面的图示中，每条线传递一个矢量，从一个节点的输出到另一个节点的输入。每个粉红色的点表示一个相应的操作，比如矢量加法，黄色的方框是用于学习的神经网络层。汇聚线表示连接，分叉线表示复制，并且复制的矢量将要流向不同的位置。&lt;/p&gt;

&lt;h2 id=&quot;lstms蕴含的核心思想&quot;&gt;LSTMs蕴含的核心思想&lt;/h2&gt;
&lt;p&gt;  LSMTs的关键在于cell状态，在下图顶部通过的水平线。cell状态像一个传送带。沿着整个链都是笔直的，仅仅会受到一些小的直线的影响。它很容易让信息沿着它流动并且保持不变。&lt;/p&gt;
&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;  LSMT根本不能给cell状态移除或添加信息，慢慢地通过门的结构来调节。&lt;/p&gt;

&lt;p&gt;  门是一种选择性的让信息通过的方式。它们由一个sigmoid神经网络层和一个对应的乘法操作组成。&lt;/p&gt;
&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 30% 20px 30%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png&quot; /&gt;
&lt;/div&gt;
&lt;p&gt;  sigmoid层输出的数据在0和1之间，描述的是每个元素允许通过的量。0表示全都不允许通过，1表示允许全部通过。&lt;/p&gt;

&lt;p&gt;  一个LSTM有三个这样的门，保存和控制cell状态。&lt;/p&gt;
&lt;h2 id=&quot;一步一步理解lstm&quot;&gt;一步一步理解LSTM&lt;/h2&gt;
&lt;p&gt;  LSTM的第一步是决定来自于cell状态的什么信息被允许通过。这个决策是通过一个称作“忘记门层”的sigmoid层制定的。它输入&lt;em&gt;\(h_{t-1}\)&lt;/em&gt;和&lt;em&gt;\(x_t\)&lt;/em&gt;,并且为cell状态中的每一个值输出一个0到1之间的数字。1表示完全保持，0表示完全阻止。&lt;/p&gt;

&lt;p&gt;  让我们回到从基于先前的所有信息预测下一个词的语言模型例子上。cell状态可能包含当前话题的类别，因而对应的名词可能被使用。当我们开始一个新的话题，我们想要忘记原来话题的类型。&lt;/p&gt;
&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  接下来是决定什么信息将要被存在cell状态。这分为两部分，首先，一个成为“输入门层”的sigmoid层决定我们将要更新哪些值，其次，一个tanh层产生了一个新的可能被添加到cell状态的候选值，&lt;em&gt;\(\widetilde{C_t}\)&lt;/em&gt;。接下来我们将结合这两者更新cell状态。&lt;/p&gt;

&lt;p&gt;  在我们的语言模型的例子中，我们想要添加新的主题来替代我们想要忘记的老的主题。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  现在该我们更新旧的cell状态&lt;em&gt;\(C_{t-1}\)&lt;/em&gt;,生成新的cell状态&lt;em&gt;\(C_t\)&lt;/em&gt;了。先前的步骤决定了怎么做，我们只需要做。&lt;/p&gt;

&lt;p&gt;  我们用&lt;em&gt;\(f_t\)&lt;/em&gt;乘以旧的cell状态，忘记我们决定忘掉的先前的事，然后加上&lt;em&gt;\(i_t*\widetilde{C_t}\)&lt;/em&gt;。这是一个候选值，由我们想要更新每个一个cell状态值多少来决定。&lt;/p&gt;

&lt;p&gt;  在语言模型的例子中，这是我们丢掉老话题的信息，添加新信息类型的地方，正如我们前面决定的。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 0% 20px 5%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  最后，我们需要决定我们将要输出什么。这个输出将依赖于cell状态，但是是过滤后的值。首先，我们生成一个sigmoid层来决定我们将要输出cell状态的哪一部分。然后我们让cell状态通过tanh层（将cell状态压缩到-1和1之间），并且乘以sigmoid门的输出，以此仅仅输出我们想要输出的部分。&lt;/p&gt;

&lt;p&gt;  对于语言模型的例子，当它看到一个主语，它可能想要与动词相关的信息，来推断下一步将要发生的事情。例如，它可能输出主语是是单数或复数，因而我们知道接下来动词应该是什么形式。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 10% 20px 10%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png&quot; /&gt;
&lt;/div&gt;

&lt;h2 id=&quot;lstm的变形&quot;&gt;LSTM的变形&lt;/h2&gt;
&lt;p&gt;  以上我们说的都是标准的LSTM模型。但是不是所有的LSTMs都是和上面的相同的。事实上，大多数与LSTMs相关的论文都是使用一个更简单的版本。难度是更小的，但是它们中 一些是值得提及的。&lt;/p&gt;
&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 5% 20px 5%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  上面的图示为每一个门添加了窥视孔，但是很多论文只把窥视孔给其中一部分而另一部分不给。&lt;/p&gt;

&lt;p&gt;  另一种变体是使用对偶的忘记门和输入门。而不是分开决定忘记什么和添加什么新信息，这俩决策是一起制定的。我们仅仅忘记我们将要输入信息的地方。我们只在往我们忘记旧的东西的时候输入新的东西。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 5% 20px 5%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  一个更大变化的LSTM变体是门控循环单元（GRU），由&lt;a href=&quot;http://arxiv.org/pdf/1406.1078v3.pdf&quot;&gt;Cho,et al.(2014)&lt;/a&gt;引进。他把忘记门和输入门融合成单独一个“更新门”。这也融合了cell状态和hidden状态，并且做了一些其他的改变。这个模型比标准LSTM模型更简单，并很快流行起来。&lt;/p&gt;

&lt;div style=&quot;margin-left:10%;margin-right:10%;padding:20px 5% 20px 5%;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  这仅仅是一些最引人注意的LSTM变体。还有很多其他的，比如&lt;a href=&quot;http://arxiv.org/pdf/1508.03790v2.pdf&quot;&gt;Yao,et al.(2015)&lt;/a&gt;的&lt;strong&gt;深度门控RNNs（Depth Gated RNNs）&lt;/strong&gt;。还有一些用来处理长期依赖问题的不同的方法，比如&lt;a href=&quot;http://arxiv.org/pdf/1402.3511v1.pdf&quot;&gt;Koutnik, et al.&lt;/a&gt;的&lt;strong&gt;Clockwork RNNs&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;  这些变体的哪一种是最好的呢？有什么关键的区别吗？&lt;a href=&quot;http://arxiv.org/pdf/1503.04069.pdf&quot;&gt;Greff, et al.(2015)&lt;/a&gt;对一些流行的变体做了很好的比较，发现它们都是相同的。&lt;a href=&quot;http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&quot;&gt;Jozefowicz, et al.(2015)&lt;/a&gt;测试了数以万计的结构，发现其中的一些比LSTMs在某些任务上效果更好。&lt;/p&gt;

&lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;

&lt;p&gt;  先前我提到了一些用RNNs实现的显著的效果。本质上这些都能用LSTMs来实现。LSTMs确实对解决很多问题都很有效。&lt;/p&gt;

&lt;p&gt;  LSTMs的公式写出来看起来非常吓人，然而，在这篇文章中一步一步的分析它们就变得更容易理解了。&lt;/p&gt;

&lt;p&gt;  在RNNs的应用上，LSTMs迈出了很大一步。我们很自然的会想，还有更大的一步么？很多研究着门一个共同的想法是：“是的，还有更大的一步并且很吸引人！”这一步指的是让RNN从具有更多信息的集合中挑选信息。这可能需要选出图像的一部分输出对应的信息。事实上，&lt;a href=&quot;http://arxiv.org/pdf/1502.03044v2.pdf&quot;&gt;Xu, et al.(2015)&lt;/a&gt;恰恰在做这个——如果你想要开发显著性，这个可能会是一个很好的开端！使用显著性会产生一些非常激动人心的结果，这似乎正在向我们走来……&lt;/p&gt;

&lt;p&gt;  在RNN的研究中显著性不仅仅是激动人心，意义深远。例如，&lt;a href=&quot;http://arxiv.org/pdf/1507.01526v1.pdf&quot;&gt;Kalchbrenner, et al. (2015)&lt;/a&gt;的网状LSTMs似乎非常有前途。在生成模型中使用RNNs——例如&lt;a href=&quot;http://arxiv.org/pdf/1502.04623.pdf&quot;&gt;Gregor, et al. (2015)&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/pdf/1506.02216v3.pdf&quot;&gt;Chung,, et al. (2015)&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/pdf/1411.7610v3.pdf&quot;&gt;Bayer &amp;amp; Osendorfer (2015)&lt;/a&gt;——也非常有意思。对于循环神经网络来说，最近这些年是非常激动人心的，未来也充满希望。&lt;/p&gt;

&lt;h2 id=&quot;致谢&quot;&gt;致谢&lt;/h2&gt;

&lt;p&gt;  我很感谢很多人帮助我更好的理解了LSTMs，可视化的评论，对这篇博文提供了反馈。非常感谢google的同事非常友好的反馈，尤其是&lt;a href=&quot;http://research.google.com/pubs/OriolVinyals.html&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://research.google.com/pubs/GregCorrado.html&quot;&gt;Greg Corrado&lt;/a&gt;, &lt;a href=&quot;http://research.google.com/pubs/JonathonShlens.html&quot;&gt;Jon Shlens&lt;/a&gt;, &lt;a href=&quot;http://people.cs.umass.edu/~luke/&quot;&gt;Luke Vilnis&lt;/a&gt;和&lt;a href=&quot;http://www.cs.toronto.edu/~ilya/&quot;&gt;Ilya Sutskever&lt;/a&gt;。同时也感谢其他花时间来帮助我的朋友和同事，包括&lt;a href=&quot;https://www.linkedin.com/pub/dario-amodei/4/493/393&quot;&gt;Dario Amodei&lt;/a&gt;, &lt;a href=&quot;http://cs.stanford.edu/~jsteinhardt/&quot;&gt;Jacob Steinhardt&lt;/a&gt;。尤其是&lt;a href=&quot;http://www.kyunghyuncho.me/&quot;&gt;Kyunghyun Cho&lt;/a&gt;对我的图示给予了非常具有思考价值的回应。&lt;/p&gt;

&lt;p&gt;  最后，在两个研讨会上，我讨论了神经网络并练习去解释LSTMs。很感谢每个人耐心的参与并给了建议。&lt;/p&gt;

&lt;hr /&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;[1].&amp;emsp;除了原有作者，还有很多人对LSTM的改进做出了贡献。所列不全：Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo和 &lt;a href=&quot;https://scholar.google.com/citations?user=DaFHynwAAAAJ&amp;amp;hl=en&quot;&gt;Alex Graves&lt;/a&gt;.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>