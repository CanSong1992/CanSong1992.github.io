<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-04-10T18:29:28+08:00</updated><id>http://localhost:4000/</id><title type="html">Can Song</title><subtitle>Can' Home</subtitle><entry><title type="html">理解LSTM网络</title><link href="http://localhost:4000/Understanding-LSTM-Networks/" rel="alternate" type="text/html" title="理解LSTM网络" /><published>2017-04-10T00:00:00+08:00</published><updated>2017-04-10T00:00:00+08:00</updated><id>http://localhost:4000/Understanding-LSTM-Networks</id><content type="html" xml:base="http://localhost:4000/Understanding-LSTM-Networks/">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;原文：&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;循环神经网络&quot;&gt;循环神经网络&lt;/h2&gt;
&lt;p&gt;  人类每一次思考不都是从头开始的。当你读这篇文章的时候，你读的每一个词都是基于以前每一个词的理解。所有的记忆都不会丢掉从头开始思考，思维都保存下来了。&lt;/p&gt;

&lt;p&gt;  传统的神经网络不能做到这些，他有一个主要的缺陷。例如，想象一下，你想要把一场电影中每一个点正在发生的事搞清楚。传统的神经网络很难从电影先前的事件从推断出后来的事情。&lt;/p&gt;

&lt;p&gt;  循环神经网络解决了这个问题。这种神经网络带有环，能够保存信息。&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-left:20%;margin-right:20%;padding:20px 25% 20px 25%;background-color:#666;&quot;&gt;&lt;!--上 右 下 左--&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png&quot; /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;循环神经网络带环&lt;/p&gt;

&lt;p&gt;  在上图中，神经网络中的方框\(A\)输入\(x_t\),输出\(h_t\)。网络中的环总是让信息从某一时刻流向下一时刻。&lt;/p&gt;

&lt;p&gt;  这些环似乎成了循环神经网络的一个迷。然而，如果你多思考一下，它们并不比传统的神经网络复杂很多。一个循环神经网络可以看成是多个相同网络的复制，每一个节点把信息传给下一个。思考一下我们把环展开的时候发生了什么：&lt;/p&gt;

&lt;div style=&quot;margin-left:20%;margin-right:20%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt;
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png&quot; /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;展开的神经网络&lt;/p&gt;

&lt;p&gt;  这个链状的结构揭示了循环神经网络与序列和列表密切相关的本质。它们是处理数据的神经网络的自然结构。&lt;/p&gt;

&lt;p&gt;  这无疑是有用的！在早些年，RNNs非常成功的应用到了这些方面：语音识别，语言建模，翻译，图像字幕……还有很多方面。我将就Andrej Karpathy优秀的博客&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;讨论他用RNNs实现的惊人成果。这真的很有意义。&lt;/p&gt;

&lt;p&gt;  这些成功的本事是“LSTMs”的使用，“LSTMs”一种特殊的循环神经网络，对于很多问题，它比标准的神经网络表现得更好。几乎所有基于循环神经网络的激动人心的成果都是用它实现的。这也是写这篇文章的原因。&lt;/p&gt;

&lt;h2 id=&quot;长时依赖问题&quot;&gt;长时依赖问题&lt;/h2&gt;
&lt;p&gt;  RNNs吸引人的原因之一是它能把先前的信息连接到当前的任务，例如把先前的视频帧的信息用于当前帧的理解。如果RNNs能做这些，它将是非常有用的。但是它能吗？并不能。&lt;/p&gt;

&lt;p&gt;  有些时候，我们仅仅需要寻找最近的信息去完成当前任务。例如，构建一个用先前信息预测下一个单词的模型。如果我们想要预测句子“The clouds are in the &lt;strong&gt;&lt;em&gt;sky&lt;/em&gt;&lt;/strong&gt;”中最后的词。我们不需要任何前面的语境——显然接下来的词就是&lt;strong&gt;&lt;em&gt;sky&lt;/em&gt;&lt;/strong&gt;。在这中情况下，所需要的相关信息和它的位置之间的间隔是很小的，RNNs能学会使用先前的信息。&lt;/p&gt;

&lt;div style=&quot;margin-left:20%;margin-right:20%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt; 
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;  但是有些场景也需要更多的上下文信息。思考预测文本“I grew up in French……I speek fluent &lt;em&gt;French&lt;/em&gt;”中最后一个单词，最近的信息表明接下来的词可能是一种语言的名字，但是我们想要搞清楚是哪一种语言，我们需要很靠前的上文Franch。很可能相关信息和自身位置之间的间隔必须是很大的。
  不幸的是，当这个间隔变得很大时，RNNs将很难学会联系相关信息。&lt;/p&gt;

&lt;div style=&quot;margin-left:20%;margin-right:20%;padding:20px 20px 20px 20px;background-color:#666;&quot;&gt; 
	&lt;img src=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png&quot; /&gt;
&lt;/div&gt;

&lt;hr /&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>